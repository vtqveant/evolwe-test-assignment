{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ptxJBvevGr3l",
        "outputId": "c1cc8716-5d73-488f-825a-1c4e4d085e4a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.21.2-py3-none-any.whl (4.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.7 MB 5.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 49.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.12.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.9.1-py3-none-any.whl (120 kB)\n",
            "\u001b[K     |████████████████████████████████| 120 kB 69.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.6.15)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.9.1 tokenizers-0.12.1 transformers-4.21.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import SubsetRandomSampler\n",
        "from torch.optim import AdamW\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "from datetime import datetime"
      ],
      "metadata": {
        "id": "00STGKKTlZy6"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_LABELS = 67"
      ],
      "metadata": {
        "id": "HtD1LapiGIqy"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LabelTracker:\n",
        "    \"\"\"A container for labels with lazy registration\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.label_idx = 0\n",
        "        self.labels = {}\n",
        "\n",
        "    def get_intent_index(self, label):\n",
        "        if label not in self.labels.keys():\n",
        "            self.labels[label] = self.label_idx\n",
        "            self.label_idx += 1\n",
        "        return self.labels[label]\n",
        "\n",
        "    def get_num_labels(self):\n",
        "        return len(self.labels)"
      ],
      "metadata": {
        "id": "3fQ87sd2Gh-i"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List, Tuple\n",
        "\n",
        "import csv\n",
        "import numpy as np\n",
        "import torch.utils.data\n",
        "from torch.utils.data.dataset import Dataset\n",
        "from torch.utils.data import SubsetRandomSampler\n",
        "\n",
        "\n",
        "class HelloEvolweDataset(Dataset):\n",
        "    def __init__(self, filename: str, label_tracker: LabelTracker):\n",
        "        super(HelloEvolweDataset, self).__init__()\n",
        "        self.label_tracker = label_tracker\n",
        "        self.filename = filename\n",
        "        self.samples = self._load()\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample = self.samples[idx]\n",
        "        return {\n",
        "            \"text\": sample[0],\n",
        "            \"intent_idx\": sample[2]\n",
        "        }\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.samples)\n",
        "\n",
        "    def _load(self) -> List[Tuple[str, str, int]]:\n",
        "        samples = []\n",
        "        with open(self.filename, 'r') as f:\n",
        "            reader = csv.DictReader(f)\n",
        "            for entry in reader:\n",
        "                samples.append((\n",
        "                    entry['text'],\n",
        "                    entry['intent'],\n",
        "                    self.label_tracker.get_intent_index(entry['intent'])\n",
        "                ))\n",
        "        return samples"
      ],
      "metadata": {
        "id": "A47qMK9TGU-6"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(args, model, tokenizer, device, train_loader, optimizer, epoch):\n",
        "    model.train()\n",
        "\n",
        "    for batch_idx, batch in enumerate(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        labels = batch['intent_idx'].to(device)\n",
        "\n",
        "        texts = batch['text']\n",
        "        encoded_input = tokenizer.batch_encode_plus(\n",
        "            batch_text_or_text_pairs=texts,\n",
        "            add_special_tokens=True,\n",
        "            padding='max_length',\n",
        "            max_length=512,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt'\n",
        "        ).to(device)\n",
        "\n",
        "        outputs = model(**encoded_input)\n",
        "        logits = outputs['logits']\n",
        "\n",
        "        criterion = torch.nn.CrossEntropyLoss()\n",
        "        loss = criterion(logits, labels)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if batch_idx % args['log_interval'] == 0:\n",
        "            print('Train epoch {} ({:.0f}%):\\tloss: {:.12f}'.format(\n",
        "                epoch, 100. * batch_idx / len(train_loader), loss.item())\n",
        "            )"
      ],
      "metadata": {
        "id": "aTIKHifwGK6e"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, tokenizer, device, test_loader):\n",
        "    model.eval()\n",
        "\n",
        "    validation_accuracy = []\n",
        "    validation_loss = []\n",
        "\n",
        "    for batch in test_loader:\n",
        "        labels = batch['intent_idx'].to(device)\n",
        "\n",
        "        texts = batch['text']\n",
        "        encoded_input = tokenizer.batch_encode_plus(\n",
        "            batch_text_or_text_pairs=texts,\n",
        "            add_special_tokens=True,\n",
        "            padding='max_length',\n",
        "            max_length=512,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt'\n",
        "        ).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**encoded_input)\n",
        "            logits = outputs['logits']\n",
        "\n",
        "        criterion = torch.nn.CrossEntropyLoss()\n",
        "        loss = criterion(logits, labels)\n",
        "        validation_loss.append(loss.item())\n",
        "\n",
        "        predictions = torch.argmax(logits, dim=1).flatten()\n",
        "        accuracy = torch.eq(predictions, labels).cpu().numpy().mean()\n",
        "        validation_accuracy.append(accuracy)\n",
        "\n",
        "    return np.mean(validation_loss), np.mean(validation_accuracy)"
      ],
      "metadata": {
        "id": "WngHYgKElvMo"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# torch.cuda.empty_cache()\n",
        "\n",
        "# training settings\n",
        "args = {\n",
        "    'batch_size': 32,\n",
        "    'epochs': 100,\n",
        "    'lr': 1e-5,\n",
        "    'log_interval': 10,\n",
        "    'snapshot_interval': 100\n",
        "}\n",
        "\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "print(f\"INFO: Using {device} device\")\n",
        "\n",
        "train_kwargs = {'batch_size': args['batch_size'], 'shuffle': False}\n",
        "if use_cuda:\n",
        "    train_kwargs.update({'num_workers': 0, 'pin_memory': True})\n",
        "\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    'bert-base-uncased',\n",
        "    num_labels=NUM_LABELS,\n",
        "    output_attentions=False,\n",
        "    output_hidden_states=False\n",
        ").to(device)\n",
        "# print(model)\n",
        "\n",
        "# freeze (some) BERT layers to avoid GPU Out-of-Memory error\n",
        "for name, param in model.named_parameters():\n",
        "    if name.startswith(\"bert.embeddings\"):\n",
        "        param.requires_grad = False\n",
        "    if name.startswith(\"bert.encoder.layer\") and not \\\n",
        "            (name.startswith(\"bert.encoder.layer.8\") or\n",
        "              name.startswith(\"bert.encoder.layer.9\") or\n",
        "              name.startswith(\"bert.encoder.layer.10\") or\n",
        "              name.startswith(\"bert.encoder.layer.11\")):\n",
        "        param.requires_grad = False\n",
        "\n",
        "# weight_decay here means L2 regularization, s. https://stackoverflow.com/questions/42704283/adding-l1-l2-regularization-in-pytorch\n",
        "# also skip frozen parameters\n",
        "optimizer = AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=args['lr'], eps=1e-8, weight_decay=1e-4)\n",
        "\n",
        "label_tracker = LabelTracker()\n",
        "dataset = HelloEvolweDataset(filename='/content/data/dataset.csv', label_tracker=label_tracker)\n",
        "\n",
        "# splits\n",
        "test_split_portion = 0.2\n",
        "n_samples = len(dataset)\n",
        "indices = list(range(n_samples))\n",
        "split_idx = int(np.floor(test_split_portion * n_samples))\n",
        "\n",
        "# shuffle\n",
        "random_seed = 42\n",
        "np.random.seed(random_seed)\n",
        "np.random.shuffle(indices)\n",
        "\n",
        "train_indices, test_indices = indices[split_idx:], indices[:split_idx]\n",
        "\n",
        "# samplers\n",
        "train_sampler = SubsetRandomSampler(train_indices)\n",
        "test_sampler = SubsetRandomSampler(test_indices)\n",
        "\n",
        "train_loader = DataLoader(dataset, sampler=train_sampler, **train_kwargs)\n",
        "test_loader = DataLoader(dataset, sampler=test_sampler, **train_kwargs)\n",
        "\n",
        "# start where we ended last time\n",
        "# model.load_state_dict(torch.load('../snapshots/03-09-2022_22:38:04_e149_lr1e-6.pth'))\n",
        "\n",
        "for epoch in range(1, args['epochs'] + 1):\n",
        "    train(args, model, tokenizer, device, train_loader, optimizer, epoch)\n",
        "    torch.save(model.state_dict(), '/content/snapshots/' + datetime.now().strftime(\"%d-%m-%Y_%H:%M:%S\") + '.pth')\n",
        "    validation_loss, validation_accuracy = evaluate(model, tokenizer, device, test_loader)\n",
        "    print(\"Eval. epoch {}:\\tloss = {:.12f}, accuracy = {:.4f}\".format(epoch, validation_loss, validation_accuracy))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iZ7rB0cvGRrv",
        "outputId": "7e963fb1-9dbf-4655-b60f-9cf7ca61cfc5"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO: Using cuda device\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train epoch 1 (0%):\tloss: 4.265549659729\n",
            "Train epoch 1 (83%):\tloss: 4.235233783722\n",
            "Eval. epoch 1:\tloss = 4.217422326406, accuracy = 0.0104\n",
            "Train epoch 2 (0%):\tloss: 4.234727382660\n",
            "Train epoch 2 (83%):\tloss: 4.160099506378\n",
            "Eval. epoch 2:\tloss = 4.199622631073, accuracy = 0.0104\n",
            "Train epoch 3 (0%):\tloss: 4.111648082733\n",
            "Train epoch 3 (83%):\tloss: 4.208429813385\n",
            "Eval. epoch 3:\tloss = 4.187750021617, accuracy = 0.0222\n",
            "Train epoch 4 (0%):\tloss: 4.162680625916\n",
            "Train epoch 4 (83%):\tloss: 4.114163398743\n",
            "Eval. epoch 4:\tloss = 4.180076281230, accuracy = 0.0312\n",
            "Train epoch 5 (0%):\tloss: 4.106890678406\n",
            "Train epoch 5 (83%):\tloss: 4.007476329803\n",
            "Eval. epoch 5:\tloss = 4.160494804382, accuracy = 0.0215\n",
            "Train epoch 6 (0%):\tloss: 4.032822132111\n",
            "Train epoch 6 (83%):\tloss: 4.025674343109\n",
            "Eval. epoch 6:\tloss = 4.140016714732, accuracy = 0.0312\n",
            "Train epoch 7 (0%):\tloss: 3.925719976425\n",
            "Train epoch 7 (83%):\tloss: 4.037945270538\n",
            "Eval. epoch 7:\tloss = 4.093083699544, accuracy = 0.0757\n",
            "Train epoch 8 (0%):\tloss: 3.920806646347\n",
            "Train epoch 8 (83%):\tloss: 4.025691509247\n",
            "Eval. epoch 8:\tloss = 4.078633467356, accuracy = 0.0639\n",
            "Train epoch 9 (0%):\tloss: 3.887301683426\n",
            "Train epoch 9 (83%):\tloss: 3.850445270538\n",
            "Eval. epoch 9:\tloss = 4.003142277400, accuracy = 0.0833\n",
            "Train epoch 10 (0%):\tloss: 3.943507909775\n",
            "Train epoch 10 (83%):\tloss: 3.631019353867\n",
            "Eval. epoch 10:\tloss = 3.965540806452, accuracy = 0.0847\n",
            "Train epoch 11 (0%):\tloss: 3.839196443558\n",
            "Train epoch 11 (83%):\tloss: 3.862946510315\n",
            "Eval. epoch 11:\tloss = 3.909374872843, accuracy = 0.1167\n",
            "Train epoch 12 (0%):\tloss: 3.690367698669\n",
            "Train epoch 12 (83%):\tloss: 3.525671482086\n",
            "Eval. epoch 12:\tloss = 3.859608888626, accuracy = 0.1278\n",
            "Train epoch 13 (0%):\tloss: 3.633927106857\n",
            "Train epoch 13 (83%):\tloss: 3.453177452087\n",
            "Eval. epoch 13:\tloss = 3.826958894730, accuracy = 0.1271\n",
            "Train epoch 14 (0%):\tloss: 3.628122091293\n",
            "Train epoch 14 (83%):\tloss: 3.751892566681\n",
            "Eval. epoch 14:\tloss = 3.744629303614, accuracy = 0.2014\n",
            "Train epoch 15 (0%):\tloss: 3.517934322357\n",
            "Train epoch 15 (83%):\tloss: 3.599358797073\n",
            "Eval. epoch 15:\tloss = 3.715966383616, accuracy = 0.1896\n",
            "Train epoch 16 (0%):\tloss: 3.454578638077\n",
            "Train epoch 16 (83%):\tloss: 3.369157791138\n",
            "Eval. epoch 16:\tloss = 3.655827999115, accuracy = 0.1910\n",
            "Train epoch 17 (0%):\tloss: 3.362454414368\n",
            "Train epoch 17 (83%):\tloss: 3.516157388687\n",
            "Eval. epoch 17:\tloss = 3.610603968302, accuracy = 0.2243\n",
            "Train epoch 18 (0%):\tloss: 3.337722539902\n",
            "Train epoch 18 (83%):\tloss: 3.313579559326\n",
            "Eval. epoch 18:\tloss = 3.569855451584, accuracy = 0.2444\n",
            "Train epoch 19 (0%):\tloss: 3.260729789734\n",
            "Train epoch 19 (83%):\tloss: 3.247516393661\n",
            "Eval. epoch 19:\tloss = 3.533318678538, accuracy = 0.2549\n",
            "Train epoch 20 (0%):\tloss: 3.328095436096\n",
            "Train epoch 20 (83%):\tloss: 3.215453624725\n",
            "Eval. epoch 20:\tloss = 3.496437311172, accuracy = 0.2549\n",
            "Train epoch 21 (0%):\tloss: 3.077107906342\n",
            "Train epoch 21 (83%):\tloss: 3.171425104141\n",
            "Eval. epoch 21:\tloss = 3.442016601562, accuracy = 0.2667\n",
            "Train epoch 22 (0%):\tloss: 2.986032962799\n",
            "Train epoch 22 (83%):\tloss: 3.243066549301\n",
            "Eval. epoch 22:\tloss = 3.406332095464, accuracy = 0.2451\n",
            "Train epoch 23 (0%):\tloss: 2.991070985794\n",
            "Train epoch 23 (83%):\tloss: 3.267627716064\n",
            "Eval. epoch 23:\tloss = 3.360906203588, accuracy = 0.2882\n",
            "Train epoch 24 (0%):\tloss: 3.062393426895\n",
            "Train epoch 24 (83%):\tloss: 2.913106441498\n",
            "Eval. epoch 24:\tloss = 3.337615807851, accuracy = 0.2646\n",
            "Train epoch 25 (0%):\tloss: 3.179857254028\n",
            "Train epoch 25 (83%):\tloss: 2.962696790695\n",
            "Eval. epoch 25:\tloss = 3.297163168589, accuracy = 0.2972\n",
            "Train epoch 26 (0%):\tloss: 2.848636150360\n",
            "Train epoch 26 (83%):\tloss: 2.784698486328\n",
            "Eval. epoch 26:\tloss = 3.249918381373, accuracy = 0.3208\n",
            "Train epoch 27 (0%):\tloss: 2.960754156113\n",
            "Train epoch 27 (83%):\tloss: 3.051618099213\n",
            "Eval. epoch 27:\tloss = 3.218714555105, accuracy = 0.2951\n",
            "Train epoch 28 (0%):\tloss: 2.910176277161\n",
            "Train epoch 28 (83%):\tloss: 2.980010986328\n",
            "Eval. epoch 28:\tloss = 3.174542029699, accuracy = 0.3292\n",
            "Train epoch 29 (0%):\tloss: 2.913880109787\n",
            "Train epoch 29 (83%):\tloss: 2.615719556808\n",
            "Eval. epoch 29:\tloss = 3.139607111613, accuracy = 0.3625\n",
            "Train epoch 30 (0%):\tloss: 2.595090389252\n",
            "Train epoch 30 (83%):\tloss: 2.521904468536\n",
            "Eval. epoch 30:\tloss = 3.101696809133, accuracy = 0.3208\n",
            "Train epoch 31 (0%):\tloss: 2.625378847122\n",
            "Train epoch 31 (83%):\tloss: 2.751269340515\n",
            "Eval. epoch 31:\tloss = 3.082738955816, accuracy = 0.3507\n",
            "Train epoch 32 (0%):\tloss: 2.501359462738\n",
            "Train epoch 32 (83%):\tloss: 2.644790649414\n",
            "Eval. epoch 32:\tloss = 3.037023226420, accuracy = 0.3521\n",
            "Train epoch 33 (0%):\tloss: 2.638786792755\n",
            "Train epoch 33 (83%):\tloss: 2.621813774109\n",
            "Eval. epoch 33:\tloss = 3.002663056056, accuracy = 0.3500\n",
            "Train epoch 34 (0%):\tloss: 2.355263233185\n",
            "Train epoch 34 (83%):\tloss: 2.467293977737\n",
            "Eval. epoch 34:\tloss = 2.956319967906, accuracy = 0.3507\n",
            "Train epoch 35 (0%):\tloss: 2.651412725449\n",
            "Train epoch 35 (83%):\tloss: 2.213048219681\n",
            "Eval. epoch 35:\tloss = 2.930717388789, accuracy = 0.3618\n",
            "Train epoch 36 (0%):\tloss: 2.474744558334\n",
            "Train epoch 36 (83%):\tloss: 2.281358003616\n",
            "Eval. epoch 36:\tloss = 2.893073717753, accuracy = 0.3826\n",
            "Train epoch 37 (0%):\tloss: 2.271090984344\n",
            "Train epoch 37 (83%):\tloss: 2.406454086304\n",
            "Eval. epoch 37:\tloss = 2.862899780273, accuracy = 0.3965\n",
            "Train epoch 38 (0%):\tloss: 2.369798898697\n",
            "Train epoch 38 (83%):\tloss: 2.146768331528\n",
            "Eval. epoch 38:\tloss = 2.834867159526, accuracy = 0.3722\n",
            "Train epoch 39 (0%):\tloss: 2.489112138748\n",
            "Train epoch 39 (83%):\tloss: 2.315889358521\n",
            "Eval. epoch 39:\tloss = 2.800280650457, accuracy = 0.3958\n",
            "Train epoch 40 (0%):\tloss: 2.305370807648\n",
            "Train epoch 40 (83%):\tloss: 2.459621667862\n",
            "Eval. epoch 40:\tloss = 2.773375670115, accuracy = 0.3826\n",
            "Train epoch 41 (0%):\tloss: 2.241686105728\n",
            "Train epoch 41 (83%):\tloss: 2.176612138748\n",
            "Eval. epoch 41:\tloss = 2.740684986115, accuracy = 0.3917\n",
            "Train epoch 42 (0%):\tloss: 2.030540227890\n",
            "Train epoch 42 (83%):\tloss: 2.129795312881\n",
            "Eval. epoch 42:\tloss = 2.714608510335, accuracy = 0.3826\n",
            "Train epoch 43 (0%):\tloss: 2.190674781799\n",
            "Train epoch 43 (83%):\tloss: 1.935735225677\n",
            "Eval. epoch 43:\tloss = 2.684199333191, accuracy = 0.3944\n",
            "Train epoch 44 (0%):\tloss: 2.067099809647\n",
            "Train epoch 44 (83%):\tloss: 2.167654991150\n",
            "Eval. epoch 44:\tloss = 2.649360497793, accuracy = 0.4444\n",
            "Train epoch 45 (0%):\tloss: 1.991728663445\n",
            "Train epoch 45 (83%):\tloss: 2.045534133911\n",
            "Eval. epoch 45:\tloss = 2.631589651108, accuracy = 0.4222\n",
            "Train epoch 46 (0%):\tloss: 2.109398126602\n",
            "Train epoch 46 (83%):\tloss: 2.055466651917\n",
            "Eval. epoch 46:\tloss = 2.595669746399, accuracy = 0.4250\n",
            "Train epoch 47 (0%):\tloss: 2.144945383072\n",
            "Train epoch 47 (83%):\tloss: 2.064548254013\n",
            "Eval. epoch 47:\tloss = 2.570854822795, accuracy = 0.4667\n",
            "Train epoch 48 (0%):\tloss: 2.106735467911\n",
            "Train epoch 48 (83%):\tloss: 1.951610922813\n",
            "Eval. epoch 48:\tloss = 2.541408061981, accuracy = 0.4819\n",
            "Train epoch 49 (0%):\tloss: 2.125272750854\n",
            "Train epoch 49 (83%):\tloss: 1.907290101051\n",
            "Eval. epoch 49:\tloss = 2.514353116353, accuracy = 0.4785\n",
            "Train epoch 50 (0%):\tloss: 1.856075644493\n",
            "Train epoch 50 (83%):\tloss: 1.861060023308\n",
            "Eval. epoch 50:\tloss = 2.494138956070, accuracy = 0.4778\n",
            "Train epoch 51 (0%):\tloss: 1.937994718552\n",
            "Train epoch 51 (83%):\tloss: 1.581614971161\n",
            "Eval. epoch 51:\tloss = 2.475651502609, accuracy = 0.4910\n",
            "Train epoch 52 (0%):\tloss: 1.749882698059\n",
            "Train epoch 52 (83%):\tloss: 1.735868096352\n",
            "Eval. epoch 52:\tloss = 2.453820943832, accuracy = 0.4979\n",
            "Train epoch 53 (0%):\tloss: 1.508020043373\n",
            "Train epoch 53 (83%):\tloss: 1.718968272209\n",
            "Eval. epoch 53:\tloss = 2.415474653244, accuracy = 0.5000\n",
            "Train epoch 54 (0%):\tloss: 1.727053165436\n",
            "Train epoch 54 (83%):\tloss: 1.633709192276\n",
            "Eval. epoch 54:\tloss = 2.388459205627, accuracy = 0.5021\n",
            "Train epoch 55 (0%):\tloss: 1.891456007957\n",
            "Train epoch 55 (83%):\tloss: 1.793335676193\n",
            "Eval. epoch 55:\tloss = 2.368468761444, accuracy = 0.5111\n",
            "Train epoch 56 (0%):\tloss: 1.682709455490\n",
            "Train epoch 56 (83%):\tloss: 1.537981152534\n",
            "Eval. epoch 56:\tloss = 2.342823664347, accuracy = 0.5104\n",
            "Train epoch 57 (0%):\tloss: 1.646718263626\n",
            "Train epoch 57 (83%):\tloss: 1.686284303665\n",
            "Eval. epoch 57:\tloss = 2.319884300232, accuracy = 0.5132\n",
            "Train epoch 58 (0%):\tloss: 1.677282214165\n",
            "Train epoch 58 (83%):\tloss: 1.434457302094\n",
            "Eval. epoch 58:\tloss = 2.294249852498, accuracy = 0.5215\n",
            "Train epoch 59 (0%):\tloss: 1.659064650536\n",
            "Train epoch 59 (83%):\tloss: 1.372014522552\n",
            "Eval. epoch 59:\tloss = 2.282999595006, accuracy = 0.5076\n",
            "Train epoch 60 (0%):\tloss: 1.524129390717\n",
            "Train epoch 60 (83%):\tloss: 1.458495020866\n",
            "Eval. epoch 60:\tloss = 2.267291943232, accuracy = 0.5326\n",
            "Train epoch 61 (0%):\tloss: 1.649120450020\n",
            "Train epoch 61 (83%):\tloss: 1.420748233795\n",
            "Eval. epoch 61:\tloss = 2.251348018646, accuracy = 0.5104\n",
            "Train epoch 62 (0%):\tloss: 1.574940919876\n",
            "Train epoch 62 (83%):\tloss: 1.368719100952\n",
            "Eval. epoch 62:\tloss = 2.229777018229, accuracy = 0.5396\n",
            "Train epoch 63 (0%):\tloss: 1.236744880676\n",
            "Train epoch 63 (83%):\tloss: 1.348316431046\n",
            "Eval. epoch 63:\tloss = 2.202541232109, accuracy = 0.5340\n",
            "Train epoch 64 (0%):\tloss: 1.186289787292\n",
            "Train epoch 64 (83%):\tloss: 1.386035680771\n",
            "Eval. epoch 64:\tloss = 2.184537410736, accuracy = 0.5424\n",
            "Train epoch 65 (0%):\tloss: 1.172358989716\n",
            "Train epoch 65 (83%):\tloss: 1.696758508682\n",
            "Eval. epoch 65:\tloss = 2.158562819163, accuracy = 0.5319\n",
            "Train epoch 66 (0%):\tloss: 1.183897852898\n",
            "Train epoch 66 (83%):\tloss: 1.393434405327\n",
            "Eval. epoch 66:\tloss = 2.138620217641, accuracy = 0.5618\n",
            "Train epoch 67 (0%):\tloss: 1.275735616684\n",
            "Train epoch 67 (83%):\tloss: 1.382154345512\n",
            "Eval. epoch 67:\tloss = 2.106513698896, accuracy = 0.5646\n",
            "Train epoch 68 (0%):\tloss: 1.090905547142\n",
            "Train epoch 68 (83%):\tloss: 1.140369415283\n",
            "Eval. epoch 68:\tloss = 2.094407995542, accuracy = 0.5535\n",
            "Train epoch 69 (0%):\tloss: 1.268060684204\n",
            "Train epoch 69 (83%):\tloss: 1.130049109459\n",
            "Eval. epoch 69:\tloss = 2.076133052508, accuracy = 0.5757\n",
            "Train epoch 70 (0%):\tloss: 1.197039246559\n",
            "Train epoch 70 (83%):\tloss: 1.297947049141\n",
            "Eval. epoch 70:\tloss = 2.059955517451, accuracy = 0.5750\n",
            "Train epoch 71 (0%):\tloss: 1.134700775146\n",
            "Train epoch 71 (83%):\tloss: 1.160846471786\n",
            "Eval. epoch 71:\tloss = 2.031742056211, accuracy = 0.5951\n",
            "Train epoch 72 (0%):\tloss: 1.019015669823\n",
            "Train epoch 72 (83%):\tloss: 1.168402194977\n",
            "Eval. epoch 72:\tloss = 2.018701672554, accuracy = 0.5868\n",
            "Train epoch 73 (0%):\tloss: 1.001190066338\n",
            "Train epoch 73 (83%):\tloss: 1.082944154739\n",
            "Eval. epoch 73:\tloss = 2.011345505714, accuracy = 0.5861\n",
            "Train epoch 74 (0%):\tloss: 0.960795402527\n",
            "Train epoch 74 (83%):\tloss: 1.198563933372\n",
            "Eval. epoch 74:\tloss = 1.988350152969, accuracy = 0.6090\n",
            "Train epoch 75 (0%):\tloss: 1.226525783539\n",
            "Train epoch 75 (83%):\tloss: 1.075719356537\n",
            "Eval. epoch 75:\tloss = 1.974532842636, accuracy = 0.5840\n",
            "Train epoch 76 (0%):\tloss: 1.144767761230\n",
            "Train epoch 76 (83%):\tloss: 1.108554482460\n",
            "Eval. epoch 76:\tloss = 1.948411822319, accuracy = 0.6056\n",
            "Train epoch 77 (0%):\tloss: 1.007256388664\n",
            "Train epoch 77 (83%):\tloss: 1.013986825943\n",
            "Eval. epoch 77:\tloss = 1.918357610703, accuracy = 0.6181\n",
            "Train epoch 78 (0%):\tloss: 0.886726915836\n",
            "Train epoch 78 (83%):\tloss: 0.956113219261\n",
            "Eval. epoch 78:\tloss = 1.907294909159, accuracy = 0.6181\n",
            "Train epoch 79 (0%):\tloss: 0.942262589931\n",
            "Train epoch 79 (83%):\tloss: 1.055551528931\n",
            "Eval. epoch 79:\tloss = 1.895976424217, accuracy = 0.6167\n",
            "Train epoch 80 (0%):\tloss: 0.953120112419\n",
            "Train epoch 80 (83%):\tloss: 0.871305048466\n",
            "Eval. epoch 80:\tloss = 1.883002400398, accuracy = 0.5931\n",
            "Train epoch 81 (0%):\tloss: 0.843766093254\n",
            "Train epoch 81 (83%):\tloss: 0.922010362148\n",
            "Eval. epoch 81:\tloss = 1.861679712931, accuracy = 0.6063\n",
            "Train epoch 82 (0%):\tloss: 0.901273071766\n",
            "Train epoch 82 (83%):\tloss: 0.801333427429\n",
            "Eval. epoch 82:\tloss = 1.849317669868, accuracy = 0.6181\n",
            "Train epoch 83 (0%):\tloss: 0.793996870518\n",
            "Train epoch 83 (83%):\tloss: 0.894958496094\n",
            "Eval. epoch 83:\tloss = 1.848742167155, accuracy = 0.6181\n",
            "Train epoch 84 (0%):\tloss: 0.851511597633\n",
            "Train epoch 84 (83%):\tloss: 0.938162684441\n",
            "Eval. epoch 84:\tloss = 1.833770910899, accuracy = 0.6153\n",
            "Train epoch 85 (0%):\tloss: 0.745998203754\n",
            "Train epoch 85 (83%):\tloss: 0.878983974457\n",
            "Eval. epoch 85:\tloss = 1.822386900584, accuracy = 0.5951\n",
            "Train epoch 86 (0%):\tloss: 0.908556580544\n",
            "Train epoch 86 (83%):\tloss: 0.887316465378\n",
            "Eval. epoch 86:\tloss = 1.803879737854, accuracy = 0.6278\n",
            "Train epoch 87 (0%):\tloss: 0.838006198406\n",
            "Train epoch 87 (83%):\tloss: 0.831380069256\n",
            "Eval. epoch 87:\tloss = 1.794130524000, accuracy = 0.6160\n",
            "Train epoch 88 (0%):\tloss: 0.784020662308\n",
            "Train epoch 88 (83%):\tloss: 0.792375802994\n",
            "Eval. epoch 88:\tloss = 1.778636654218, accuracy = 0.6167\n",
            "Train epoch 89 (0%):\tloss: 0.741487503052\n",
            "Train epoch 89 (83%):\tloss: 0.936127066612\n",
            "Eval. epoch 89:\tloss = 1.766432921092, accuracy = 0.6174\n",
            "Train epoch 90 (0%):\tloss: 0.717379570007\n",
            "Train epoch 90 (83%):\tloss: 0.806341767311\n",
            "Eval. epoch 90:\tloss = 1.751361290614, accuracy = 0.6153\n",
            "Train epoch 91 (0%):\tloss: 0.604339718819\n",
            "Train epoch 91 (83%):\tloss: 0.834479093552\n",
            "Eval. epoch 91:\tloss = 1.743703524272, accuracy = 0.6146\n",
            "Train epoch 92 (0%):\tloss: 0.890120685101\n",
            "Train epoch 92 (83%):\tloss: 0.738360822201\n",
            "Eval. epoch 92:\tloss = 1.724349975586, accuracy = 0.6049\n",
            "Train epoch 93 (0%):\tloss: 0.745216310024\n",
            "Train epoch 93 (83%):\tloss: 0.594833850861\n",
            "Eval. epoch 93:\tloss = 1.706078886986, accuracy = 0.6403\n",
            "Train epoch 94 (0%):\tloss: 0.738867759705\n",
            "Train epoch 94 (83%):\tloss: 0.564829587936\n",
            "Eval. epoch 94:\tloss = 1.683872421583, accuracy = 0.6306\n",
            "Train epoch 95 (0%):\tloss: 0.732425570488\n",
            "Train epoch 95 (83%):\tloss: 0.546041846275\n",
            "Eval. epoch 95:\tloss = 1.688811182976, accuracy = 0.6174\n",
            "Train epoch 96 (0%):\tloss: 0.762187302113\n",
            "Train epoch 96 (83%):\tloss: 0.623849332333\n",
            "Eval. epoch 96:\tloss = 1.686773061752, accuracy = 0.6243\n",
            "Train epoch 97 (0%):\tloss: 0.691178381443\n",
            "Train epoch 97 (83%):\tloss: 0.672770619392\n",
            "Eval. epoch 97:\tloss = 1.669163227081, accuracy = 0.6292\n",
            "Train epoch 98 (0%):\tloss: 0.752552986145\n",
            "Train epoch 98 (83%):\tloss: 0.575711965561\n",
            "Eval. epoch 98:\tloss = 1.657690445582, accuracy = 0.6153\n",
            "Train epoch 99 (0%):\tloss: 0.707665324211\n",
            "Train epoch 99 (83%):\tloss: 0.578413963318\n",
            "Eval. epoch 99:\tloss = 1.631407022476, accuracy = 0.6396\n",
            "Train epoch 100 (0%):\tloss: 0.636148273945\n",
            "Train epoch 100 (83%):\tloss: 0.692250907421\n",
            "Eval. epoch 100:\tloss = 1.610830585162, accuracy = 0.6410\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Hurs4EduqZKs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}