{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ptxJBvevGr3l",
        "outputId": "7035f726-6d2f-41e7-e79a-c618c9c1c5b3"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.21.2-py3-none-any.whl (4.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.7 MB 29.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
            "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 69.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.12.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.9.1-py3-none-any.whl (120 kB)\n",
            "\u001b[K     |████████████████████████████████| 120 kB 73.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.6.15)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.9.1 tokenizers-0.12.1 transformers-4.21.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_LABELS = 71"
      ],
      "metadata": {
        "id": "HtD1LapiGIqy"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import abc\n",
        "\n",
        "\n",
        "class LabelTracker(metaclass=abc.ABCMeta):\n",
        "\n",
        "    @classmethod\n",
        "    def __subclasshook__(cls, subclass):\n",
        "        return (hasattr(subclass, 'get_intent_index') and callable(subclass.get_intent_index) or NotImplemented)\n",
        "\n",
        "    @abc.abstractmethod\n",
        "    def get_intent_index(self, language: str) -> int:\n",
        "        raise NotImplementedError\n",
        "\n",
        "\n",
        "class DictLabelTracker(LabelTracker):\n",
        "    \"\"\"A container for labels with lazy registration\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.intent_index = 0\n",
        "        self.intents = {}\n",
        "\n",
        "    def get_intent_index(self, intent):\n",
        "        if intent not in self.intents.keys():\n",
        "            self.intents[intent] = self.intent_index\n",
        "            self.intent_index += 1\n",
        "        return self.intents[intent]\n"
      ],
      "metadata": {
        "id": "3fQ87sd2Gh-i"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import yaml\n",
        "import random\n",
        "from torch.utils.data.dataset import IterableDataset\n",
        "\n",
        "\n",
        "class HelloEvolweDataset(IterableDataset):\n",
        "    def __init__(self, filename: str, label_tracker: LabelTracker, shuffle=True):\n",
        "        super(HelloEvolweDataset, self).__init__()\n",
        "        self.label_tracker = label_tracker\n",
        "        self.filename = filename\n",
        "        self.samples = self._load()\n",
        "        if shuffle:\n",
        "            random.shuffle(self.samples)\n",
        "\n",
        "\n",
        "    def __iter__(self):\n",
        "        for i, row in enumerate(self.samples):\n",
        "            yield {\n",
        "                # \"id\": i,\n",
        "                \"text\": row[0],\n",
        "                # \"intent\": row[1],\n",
        "                \"intent_idx\": self.label_tracker.get_intent_index(row[1])\n",
        "            }\n",
        "\n",
        "    def _load(self):\n",
        "        samples = []\n",
        "        with open(self.filename, 'r') as file:\n",
        "            documents = yaml.full_load(file)\n",
        "            for entry in documents['data']:\n",
        "                intent = entry['intent']\n",
        "                for example in entry['examples']:\n",
        "                    samples.append((example, intent))\n",
        "        return samples\n",
        "\n",
        "    def __len__(self):\n",
        "      return len(self.samples)\n"
      ],
      "metadata": {
        "id": "A47qMK9TGU-6"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "9_2kvsktGDwB"
      },
      "outputs": [],
      "source": [
        "import argparse\n",
        "from datetime import datetime\n",
        "\n",
        "import torch\n",
        "from torch.optim import Adam\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import BertTokenizer, BertForSequenceClassification"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train(args, model, tokenizer, device, train_loader, optimizer, epoch):\n",
        "    model.train()\n",
        "    for batch_idx, sample in enumerate(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        labels = sample['intent_idx'].unsqueeze(0).to(device)\n",
        "\n",
        "        texts = sample['text']\n",
        "        encoded_input = tokenizer.batch_encode_plus(\n",
        "            batch_text_or_text_pairs=texts,\n",
        "            add_special_tokens=True,\n",
        "            padding='max_length',\n",
        "            max_length=512,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt'\n",
        "        ).to(device)\n",
        "\n",
        "        outputs = model(**encoded_input, labels=labels)\n",
        "        loss, logits = outputs[:2]\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if batch_idx % args.log_interval == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.12f}'.format(\n",
        "                epoch, batch_idx * len(texts), len(train_loader.dataset),\n",
        "                       100. * batch_idx / len(train_loader), loss.item()))\n",
        "            if args.dry_run:\n",
        "                break"
      ],
      "metadata": {
        "id": "aTIKHifwGK6e"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Struct:\n",
        "    def __init__(self, **entries):\n",
        "        self.__dict__.update(entries)"
      ],
      "metadata": {
        "id": "oPmU7MOENEao"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "ElWarxVVQA7a"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# training settings\n",
        "args = Struct(**{\n",
        "    'batch_size': 15,\n",
        "    'epochs': 20,\n",
        "    'lr': 0.0001,\n",
        "    'log_interval': 10,\n",
        "    'dry_run': False,\n",
        "    'snapshot_interval': 50\n",
        "})\n",
        "\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "print(f\"INFO: Using {device} device\")\n",
        "\n",
        "train_kwargs = {'batch_size': args.batch_size, 'shuffle': False}\n",
        "if use_cuda:\n",
        "    train_kwargs.update({'num_workers': 0, 'pin_memory': True})\n",
        "\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    'bert-base-uncased',\n",
        "    num_labels=NUM_LABELS,\n",
        "    output_attentions=False,\n",
        "    output_hidden_states=False\n",
        ").to(device)\n",
        "# print(model)\n",
        "\n",
        "# weight_decay here means L2 regularization, s. https://stackoverflow.com/questions/42704283/adding-l1-l2-regularization-in-pytorch\n",
        "optimizer = Adam(model.parameters(), lr=args.lr, weight_decay=1e-5)\n",
        "\n",
        "train_dataset = HelloEvolweDataset(\n",
        "    filename='data/hello_nova_intents_0.2.2.yaml',\n",
        "    label_tracker=DictLabelTracker(),\n",
        "    shuffle=True\n",
        ")\n",
        "train_loader = DataLoader(train_dataset, **train_kwargs)\n",
        "\n",
        "for epoch in range(1, args.epochs + 1):\n",
        "    train(args, model, tokenizer, device, train_loader, optimizer, epoch)\n",
        "    torch.save(model.state_dict(), 'snapshots/' + datetime.now().strftime(\"%d-%m-%Y_%H:%M:%S\") + '.pth')\n",
        "    # test(model, device, test_loader)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iZ7rB0cvGRrv",
        "outputId": "24cbf18e-b552-4dca-a565-6fb09bb92b77"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO: Using cuda device\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 1 [0/483 (0%)]\tLoss: 4.266362190247\n",
            "Train Epoch: 1 [150/483 (30%)]\tLoss: 4.371157646179\n",
            "Train Epoch: 1 [300/483 (61%)]\tLoss: 4.353417873383\n",
            "Train Epoch: 1 [450/483 (91%)]\tLoss: 4.037541389465\n",
            "Train Epoch: 2 [0/483 (0%)]\tLoss: 4.126181602478\n",
            "Train Epoch: 2 [150/483 (30%)]\tLoss: 4.133839130402\n",
            "Train Epoch: 2 [300/483 (61%)]\tLoss: 4.395199775696\n",
            "Train Epoch: 2 [450/483 (91%)]\tLoss: 3.941416978836\n",
            "Train Epoch: 3 [0/483 (0%)]\tLoss: 4.000207901001\n",
            "Train Epoch: 3 [150/483 (30%)]\tLoss: 3.906985759735\n",
            "Train Epoch: 3 [300/483 (61%)]\tLoss: 3.997283458710\n",
            "Train Epoch: 3 [450/483 (91%)]\tLoss: 3.265368700027\n",
            "Train Epoch: 4 [0/483 (0%)]\tLoss: 3.703966379166\n",
            "Train Epoch: 4 [150/483 (30%)]\tLoss: 3.509290218353\n",
            "Train Epoch: 4 [300/483 (61%)]\tLoss: 3.809411048889\n",
            "Train Epoch: 4 [450/483 (91%)]\tLoss: 2.681430578232\n",
            "Train Epoch: 5 [0/483 (0%)]\tLoss: 3.115521669388\n",
            "Train Epoch: 5 [150/483 (30%)]\tLoss: 3.069729328156\n",
            "Train Epoch: 5 [300/483 (61%)]\tLoss: 2.964859485626\n",
            "Train Epoch: 5 [450/483 (91%)]\tLoss: 2.041087150574\n",
            "Train Epoch: 6 [0/483 (0%)]\tLoss: 2.530998945236\n",
            "Train Epoch: 6 [150/483 (30%)]\tLoss: 2.414076328278\n",
            "Train Epoch: 6 [300/483 (61%)]\tLoss: 2.243234395981\n",
            "Train Epoch: 6 [450/483 (91%)]\tLoss: 1.381231069565\n",
            "Train Epoch: 7 [0/483 (0%)]\tLoss: 1.880353569984\n",
            "Train Epoch: 7 [150/483 (30%)]\tLoss: 1.607820034027\n",
            "Train Epoch: 7 [300/483 (61%)]\tLoss: 1.481820344925\n",
            "Train Epoch: 7 [450/483 (91%)]\tLoss: 0.984571635723\n",
            "Train Epoch: 8 [0/483 (0%)]\tLoss: 1.366584539413\n",
            "Train Epoch: 8 [150/483 (30%)]\tLoss: 1.144621253014\n",
            "Train Epoch: 8 [300/483 (61%)]\tLoss: 0.998052537441\n",
            "Train Epoch: 8 [450/483 (91%)]\tLoss: 0.695989191532\n",
            "Train Epoch: 9 [0/483 (0%)]\tLoss: 0.921846568584\n",
            "Train Epoch: 9 [150/483 (30%)]\tLoss: 0.849903345108\n",
            "Train Epoch: 9 [300/483 (61%)]\tLoss: 0.592875123024\n",
            "Train Epoch: 9 [450/483 (91%)]\tLoss: 0.474124222994\n",
            "Train Epoch: 10 [0/483 (0%)]\tLoss: 0.672773063183\n",
            "Train Epoch: 10 [150/483 (30%)]\tLoss: 0.640276670456\n",
            "Train Epoch: 10 [300/483 (61%)]\tLoss: 0.451328843832\n",
            "Train Epoch: 10 [450/483 (91%)]\tLoss: 0.364521473646\n",
            "Train Epoch: 11 [0/483 (0%)]\tLoss: 0.580500900745\n",
            "Train Epoch: 11 [150/483 (30%)]\tLoss: 0.466476947069\n",
            "Train Epoch: 11 [300/483 (61%)]\tLoss: 0.300308525562\n",
            "Train Epoch: 11 [450/483 (91%)]\tLoss: 0.251158565283\n",
            "Train Epoch: 12 [0/483 (0%)]\tLoss: 0.320675522089\n",
            "Train Epoch: 12 [150/483 (30%)]\tLoss: 0.304525494576\n",
            "Train Epoch: 12 [300/483 (61%)]\tLoss: 0.248319447041\n",
            "Train Epoch: 12 [450/483 (91%)]\tLoss: 0.189366802573\n",
            "Train Epoch: 13 [0/483 (0%)]\tLoss: 0.240122407675\n",
            "Train Epoch: 13 [150/483 (30%)]\tLoss: 0.273160606623\n",
            "Train Epoch: 13 [300/483 (61%)]\tLoss: 0.211122617126\n",
            "Train Epoch: 13 [450/483 (91%)]\tLoss: 0.173197090626\n",
            "Train Epoch: 14 [0/483 (0%)]\tLoss: 0.201315015554\n",
            "Train Epoch: 14 [150/483 (30%)]\tLoss: 0.280164539814\n",
            "Train Epoch: 14 [300/483 (61%)]\tLoss: 0.123638562858\n",
            "Train Epoch: 14 [450/483 (91%)]\tLoss: 0.111451320350\n",
            "Train Epoch: 15 [0/483 (0%)]\tLoss: 0.134496390820\n",
            "Train Epoch: 15 [150/483 (30%)]\tLoss: 0.187437638640\n",
            "Train Epoch: 15 [300/483 (61%)]\tLoss: 0.102362386882\n",
            "Train Epoch: 15 [450/483 (91%)]\tLoss: 0.094602175057\n",
            "Train Epoch: 16 [0/483 (0%)]\tLoss: 0.120180591941\n",
            "Train Epoch: 16 [150/483 (30%)]\tLoss: 0.180696323514\n",
            "Train Epoch: 16 [300/483 (61%)]\tLoss: 0.084091417491\n",
            "Train Epoch: 16 [450/483 (91%)]\tLoss: 0.070596858859\n",
            "Train Epoch: 17 [0/483 (0%)]\tLoss: 0.102013297379\n",
            "Train Epoch: 17 [150/483 (30%)]\tLoss: 0.131638631225\n",
            "Train Epoch: 17 [300/483 (61%)]\tLoss: 0.074210412800\n",
            "Train Epoch: 17 [450/483 (91%)]\tLoss: 0.061730179936\n",
            "Train Epoch: 18 [0/483 (0%)]\tLoss: 0.075300902128\n",
            "Train Epoch: 18 [150/483 (30%)]\tLoss: 0.186901122332\n",
            "Train Epoch: 18 [300/483 (61%)]\tLoss: 0.065301150084\n",
            "Train Epoch: 18 [450/483 (91%)]\tLoss: 0.050813779235\n",
            "Train Epoch: 19 [0/483 (0%)]\tLoss: 0.078775614500\n",
            "Train Epoch: 19 [150/483 (30%)]\tLoss: 0.178191021085\n",
            "Train Epoch: 19 [300/483 (61%)]\tLoss: 0.055909164250\n",
            "Train Epoch: 19 [450/483 (91%)]\tLoss: 0.046847928315\n",
            "Train Epoch: 20 [0/483 (0%)]\tLoss: 0.059935703874\n",
            "Train Epoch: 20 [150/483 (30%)]\tLoss: 0.171243086457\n",
            "Train Epoch: 20 [300/483 (61%)]\tLoss: 0.051299944520\n",
            "Train Epoch: 20 [450/483 (91%)]\tLoss: 0.040128063411\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -laFh snapshots"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qZLXIthCGcRw",
        "outputId": "2556f7cd-aa3e-4f3f-fe53-505fd3aed2d3"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 8.6G\n",
            "drwxr-xr-x 3 root root 4.0K Sep  2 18:34 ./\n",
            "drwxr-xr-x 1 root root 4.0K Sep  2 17:33 ../\n",
            "-rw-r--r-- 1 root root 418M Sep  2 18:17 02-09-2022_18:17:55.pth\n",
            "-rw-r--r-- 1 root root 418M Sep  2 18:19 02-09-2022_18:19:27.pth\n",
            "-rw-r--r-- 1 root root 418M Sep  2 18:20 02-09-2022_18:20:14.pth\n",
            "-rw-r--r-- 1 root root 418M Sep  2 18:21 02-09-2022_18:21:00.pth\n",
            "-rw-r--r-- 1 root root 418M Sep  2 18:21 02-09-2022_18:21:47.pth\n",
            "-rw-r--r-- 1 root root 418M Sep  2 18:22 02-09-2022_18:22:34.pth\n",
            "-rw-r--r-- 1 root root 418M Sep  2 18:23 02-09-2022_18:23:20.pth\n",
            "-rw-r--r-- 1 root root 418M Sep  2 18:24 02-09-2022_18:24:07.pth\n",
            "-rw-r--r-- 1 root root 418M Sep  2 18:24 02-09-2022_18:24:53.pth\n",
            "-rw-r--r-- 1 root root 418M Sep  2 18:25 02-09-2022_18:25:40.pth\n",
            "-rw-r--r-- 1 root root 418M Sep  2 18:26 02-09-2022_18:26:26.pth\n",
            "-rw-r--r-- 1 root root 418M Sep  2 18:27 02-09-2022_18:27:13.pth\n",
            "-rw-r--r-- 1 root root 418M Sep  2 18:28 02-09-2022_18:27:59.pth\n",
            "-rw-r--r-- 1 root root 418M Sep  2 18:28 02-09-2022_18:28:46.pth\n",
            "-rw-r--r-- 1 root root 418M Sep  2 18:29 02-09-2022_18:29:32.pth\n",
            "-rw-r--r-- 1 root root 418M Sep  2 18:30 02-09-2022_18:30:18.pth\n",
            "-rw-r--r-- 1 root root 418M Sep  2 18:31 02-09-2022_18:31:05.pth\n",
            "-rw-r--r-- 1 root root 418M Sep  2 18:31 02-09-2022_18:31:52.pth\n",
            "-rw-r--r-- 1 root root 418M Sep  2 18:32 02-09-2022_18:32:38.pth\n",
            "-rw-r--r-- 1 root root 418M Sep  2 18:33 02-09-2022_18:33:24.pth\n",
            "-rw-r--r-- 1 root root 418M Sep  2 18:34 02-09-2022_18:34:11.pth\n",
            "drwxr-xr-x 2 root root 4.0K Sep  2 17:38 .ipynb_checkpoints/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gdcqkvwMfML_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}