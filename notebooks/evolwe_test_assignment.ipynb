{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ptxJBvevGr3l",
        "outputId": "8a3caded-8f7a-42de-ef63-6371eec335b3"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.21.2-py3-none-any.whl (4.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.7 MB 5.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.9.1-py3-none-any.whl (120 kB)\n",
            "\u001b[K     |████████████████████████████████| 120 kB 68.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.12.0)\n",
            "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 48.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.6.15)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.9.1 tokenizers-0.12.1 transformers-4.21.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_LABELS = 67"
      ],
      "metadata": {
        "id": "HtD1LapiGIqy"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LabelTracker:\n",
        "    \"\"\"A container for labels with lazy registration\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.label_idx = 0\n",
        "        self.labels = {}\n",
        "\n",
        "    def get_intent_index(self, label):\n",
        "        if label not in self.labels.keys():\n",
        "            self.labels[label] = self.label_idx\n",
        "            self.label_idx += 1\n",
        "        return self.labels[label]\n",
        "\n",
        "    def get_num_labels(self):\n",
        "        return len(self.labels)"
      ],
      "metadata": {
        "id": "3fQ87sd2Gh-i"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List, Tuple\n",
        "\n",
        "import csv\n",
        "from torch.utils.data.dataset import Dataset\n",
        "\n",
        "\n",
        "class HelloEvolweDataset(Dataset):\n",
        "    def __init__(self, filename: str, label_tracker: LabelTracker):\n",
        "        super(HelloEvolweDataset, self).__init__()\n",
        "        self.label_tracker = label_tracker\n",
        "        self.filename = filename\n",
        "        self.samples = self._load()\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample = self.samples[idx]\n",
        "        return {\n",
        "            \"text\": sample[0],\n",
        "            \"intent_idx\": sample[2]\n",
        "        }\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.samples)\n",
        "\n",
        "    def get_class_weights(self):\n",
        "        n_classes = self.label_tracker.get_num_labels()\n",
        "        n_samples = [0 for _ in range(n_classes)]\n",
        "        for sample in self.samples:\n",
        "            i = self.label_tracker.get_intent_index(sample[1])\n",
        "            n_samples[i] += 1\n",
        "        weights = [count / n_classes for count in n_samples]\n",
        "        return weights\n",
        "\n",
        "    def _load(self) -> List[Tuple[str, str, int]]:\n",
        "        samples = []\n",
        "        with open(self.filename, 'r') as f:\n",
        "            reader = csv.DictReader(f)\n",
        "            for entry in reader:\n",
        "                samples.append((\n",
        "                    entry['text'],\n",
        "                    entry['intent'],\n",
        "                    self.label_tracker.get_intent_index(entry['intent'])\n",
        "                ))\n",
        "        return samples"
      ],
      "metadata": {
        "id": "A47qMK9TGU-6"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "id": "9_2kvsktGDwB"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n",
        "from datetime import datetime"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train(args, model, tokenizer, device, train_loader, optimizer, scheduler, epoch, class_weights):\n",
        "    model.train()\n",
        "    class_weights = torch.tensor(class_weights).to(device)\n",
        "\n",
        "    for batch_idx, sample in enumerate(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        labels = sample['intent_idx'].to(device)\n",
        "\n",
        "        texts = sample['text']\n",
        "        encoded_input = tokenizer.batch_encode_plus(\n",
        "            batch_text_or_text_pairs=texts,\n",
        "            add_special_tokens=True,\n",
        "            padding='max_length',\n",
        "            max_length=512,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt'\n",
        "        ).to(device)\n",
        "\n",
        "        outputs = model(**encoded_input)\n",
        "        logits = outputs['logits']\n",
        "\n",
        "        criterion = torch.nn.CrossEntropyLoss(weight=class_weights)\n",
        "        loss = criterion(logits, labels)\n",
        "\n",
        "        loss.backward()\n",
        "        \n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "        if batch_idx % args['log_interval'] == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.12f}'.format(\n",
        "                epoch, batch_idx * len(texts), len(train_loader.dataset),\n",
        "                       100. * batch_idx / len(train_loader), loss.item()))\n",
        "            if args['dry_run']:\n",
        "                break"
      ],
      "metadata": {
        "id": "aTIKHifwGK6e"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "ElWarxVVQA7a"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# training settings\n",
        "args = {\n",
        "    'batch_size': 15,\n",
        "    'epochs': 20,\n",
        "    'lr': 5e-5,\n",
        "    'log_interval': 10,\n",
        "    'dry_run': False,\n",
        "    'snapshot_interval': 100\n",
        "}\n",
        "\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "print(f\"INFO: Using {device} device\")\n",
        "\n",
        "train_kwargs = {'batch_size': args['batch_size'], 'shuffle': True}\n",
        "if use_cuda:\n",
        "    train_kwargs.update({'num_workers': 0, 'pin_memory': True})\n",
        "\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    'bert-base-uncased',\n",
        "    num_labels=NUM_LABELS,\n",
        "    output_attentions=False,\n",
        "    output_hidden_states=False\n",
        ").to(device)\n",
        "# print(model)\n",
        "\n",
        "# weight_decay here means L2 regularization, s. https://stackoverflow.com/questions/42704283/adding-l1-l2-regularization-in-pytorch\n",
        "optimizer = AdamW(model.parameters(), lr=args['lr'], eps=1e-8, weight_decay=1e-4)\n",
        "\n",
        "train_dataset = HelloEvolweDataset(\n",
        "    filename='data/train.csv',\n",
        "    label_tracker=LabelTracker()\n",
        ")\n",
        "train_loader = DataLoader(train_dataset, **train_kwargs)\n",
        "class_weights = train_dataset.get_class_weights()\n",
        "\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=args['epochs'])\n",
        "\n",
        "# start where we ended last time\n",
        "# model.load_state_dict(torch.load('/content/snapshots/02-09-2022_19:01:31.pth'))\n",
        "\n",
        "for epoch in range(1, args['epochs'] + 1):\n",
        "    train(args, model, tokenizer, device, train_loader, optimizer, scheduler, epoch, class_weights)\n",
        "    torch.save(model.state_dict(), 'snapshots/' + datetime.now().strftime(\"%d-%m-%Y_%H:%M:%S\") + '.pth')\n",
        "    # test(model, device, test_loader)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iZ7rB0cvGRrv",
        "outputId": "7a90956f-e071-4ebd-b69d-e75b639e24e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO: Using cuda device\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 1 [0/472 (0%)]\tLoss: 4.307265281677\n",
            "Train Epoch: 1 [150/472 (31%)]\tLoss: 3.987587451935\n",
            "Train Epoch: 1 [300/472 (62%)]\tLoss: 4.265531539917\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -laFh snapshots"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qZLXIthCGcRw",
        "outputId": "2556f7cd-aa3e-4f3f-fe53-505fd3aed2d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 8.6G\n",
            "drwxr-xr-x 3 root root 4.0K Sep  2 18:34 ./\n",
            "drwxr-xr-x 1 root root 4.0K Sep  2 17:33 ../\n",
            "-rw-r--r-- 1 root root 418M Sep  2 18:17 02-09-2022_18:17:55.pth\n",
            "-rw-r--r-- 1 root root 418M Sep  2 18:19 02-09-2022_18:19:27.pth\n",
            "-rw-r--r-- 1 root root 418M Sep  2 18:20 02-09-2022_18:20:14.pth\n",
            "-rw-r--r-- 1 root root 418M Sep  2 18:21 02-09-2022_18:21:00.pth\n",
            "-rw-r--r-- 1 root root 418M Sep  2 18:21 02-09-2022_18:21:47.pth\n",
            "-rw-r--r-- 1 root root 418M Sep  2 18:22 02-09-2022_18:22:34.pth\n",
            "-rw-r--r-- 1 root root 418M Sep  2 18:23 02-09-2022_18:23:20.pth\n",
            "-rw-r--r-- 1 root root 418M Sep  2 18:24 02-09-2022_18:24:07.pth\n",
            "-rw-r--r-- 1 root root 418M Sep  2 18:24 02-09-2022_18:24:53.pth\n",
            "-rw-r--r-- 1 root root 418M Sep  2 18:25 02-09-2022_18:25:40.pth\n",
            "-rw-r--r-- 1 root root 418M Sep  2 18:26 02-09-2022_18:26:26.pth\n",
            "-rw-r--r-- 1 root root 418M Sep  2 18:27 02-09-2022_18:27:13.pth\n",
            "-rw-r--r-- 1 root root 418M Sep  2 18:28 02-09-2022_18:27:59.pth\n",
            "-rw-r--r-- 1 root root 418M Sep  2 18:28 02-09-2022_18:28:46.pth\n",
            "-rw-r--r-- 1 root root 418M Sep  2 18:29 02-09-2022_18:29:32.pth\n",
            "-rw-r--r-- 1 root root 418M Sep  2 18:30 02-09-2022_18:30:18.pth\n",
            "-rw-r--r-- 1 root root 418M Sep  2 18:31 02-09-2022_18:31:05.pth\n",
            "-rw-r--r-- 1 root root 418M Sep  2 18:31 02-09-2022_18:31:52.pth\n",
            "-rw-r--r-- 1 root root 418M Sep  2 18:32 02-09-2022_18:32:38.pth\n",
            "-rw-r--r-- 1 root root 418M Sep  2 18:33 02-09-2022_18:33:24.pth\n",
            "-rw-r--r-- 1 root root 418M Sep  2 18:34 02-09-2022_18:34:11.pth\n",
            "drwxr-xr-x 2 root root 4.0K Sep  2 17:38 .ipynb_checkpoints/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mv /content/snapshots/03-09-2022_06:53:41.pth /content/drive/MyDrive/Colab\\ Snapshots/evelowe_test_assignment"
      ],
      "metadata": {
        "id": "gdcqkvwMfML_"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Hurs4EduqZKs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}